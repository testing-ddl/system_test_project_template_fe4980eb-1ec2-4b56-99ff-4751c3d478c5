{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install --user datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:17:10.121092Z",
     "start_time": "2022-06-02T02:17:10.098888Z"
    }
   },
   "source": [
    "# Fine-tuning FinBERT for sentiment analysis of financial news\n",
    "\n",
    "FinBERT is a BERT model pre-trained on financial communication text. It has been shown that FinBERT outperforms traditional machine learning models on several financial NLP tasks [1]. The model is trained on a total corpora size of 4.9B tokens, and is available in the following flavours (all hosted at Huggingface ðŸ¤—):\n",
    "\n",
    "* FinBERT-Pretrained: The pretrained FinBERT model on large-scale financial text.\n",
    "* FinBERT-Sentiment: for sentiment classification task.\n",
    "* FinBERT-ESG: for ESG classification task.\n",
    "* FinBERT-FLS: for forward-looking statement (FLS) classification task.\n",
    "\n",
    "This notebook uses code from [FinBERT.AI](https://finbert.ai/) to showcase the use of pre-trained models in Domino and to demonstrate the process of GPU-accelerated fine-tuning using Nvidia GPUs. We also use the [Sentiment Analysis for Financial News](https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news) dataset [2], which provides 4,837 samples of sentiments for financial news headlines from the perspective of a retail investor.\n",
    "\n",
    "*[1] Yi Yang and Mark Christopher Siy UY and Allen Huang, FinBERT: A Pretrained Language Model for Financial Communications, 2020, [2006.08097](https://arxiv.org/abs/2006.08097)*\n",
    "\n",
    "*[2] Malo, P., Sinha, A., Korhonen, P., Wallenius, J., & Takala, P. (2014). Good debt or bad debt: Detecting semantic orientations in economic texts. Journal of the Association for Information Science and Technology, 65(4), 782-796.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple demonstration of FinBERT\n",
    "\n",
    "Let's start by loading the libraries that are needed for acessing and fine-tuning FinBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:30:37.970878Z",
     "start_time": "2022-06-02T02:30:34.740917Z"
    }
   },
   "outputs": [],
   "source": [
    "import nvidia\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "from transformers import BertTokenizer, Trainer, BertForSequenceClassification, TrainingArguments, pipeline\n",
    "from transformers import enable_full_determinism\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_install_dir = '/'.join(nvidia.__file__.split('/')[:-1]) + '/cuda_runtime/lib/'\n",
    "os.environ['LD_LIBRARY_PATH'] =  cuda_install_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure GPU acceleration is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:44:18.286115Z",
     "start_time": "2022-06-02T02:44:18.222850Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU acceleration is available!\")\n",
    "else:\n",
    "    print(\"GPU acceleration is NOT available! Training, fine-tuning, and inference speed will be adversely impacted.\")\n",
    "    \n",
    "enable_full_determinism(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load FinBERT and classify a handful of test statments. The NLP pipeline produces a label and a prediction score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\",num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "sentences = [\"there is a shortage of capital, and we need extra financing\",  \n",
    "             \"growth is strong and we have plenty of liquidity\", \n",
    "             \"there are doubts about our finances\", \n",
    "             \"profits are flat\"]\n",
    "results = nlp(sentences)\n",
    "\n",
    "for sample in zip(sentences, results):\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Financial news headlines dataset\n",
    "\n",
    "Let's now load the Financial news dataset. The dataset has two attributes:\n",
    "\n",
    "* **sentence** - the news headline\n",
    "* **label** - sentiment, which we will encode as follows:\n",
    "    * neutral  : 0\n",
    "    * positive : 1\n",
    "    * negative : 2\n",
    "    \n",
    "Let's process the dataset and show the first 5 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from CSV\n",
    "df = pd.read_csv(\"all-data.csv\", delimiter=\",\", encoding=\"latin-1\", header=None).fillna(\"\")\n",
    "df = df.rename(columns=lambda x: [\"label\", \"sentence\"][x])\n",
    "\n",
    "# Encode labels\n",
    "df[\"label\"] = df[\"label\"].replace([\"neutral\",\"positive\",\"negative\"],[0,1,2]) \n",
    "\n",
    "# Print first 5\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that there are no missing value in the data. We can now proceed with splitting it into a training, test, and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentence\"].map(len).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training, test, and validation subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the training dataset into a training, test, and validation subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:54:08.030875Z",
     "start_time": "2022-06-02T02:54:07.999354Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train, df_test, = train_test_split(df, stratify=df[\"label\"], test_size=0.1, random_state=42)\n",
    "df_train, df_val = train_test_split(df_train, stratify=df_train[\"label\"],test_size=0.1, random_state=42)\n",
    "print(\"Samples in train      : {:d}\".format(df_train.shape[0]))\n",
    "print(\"Samples in validation : {:d}\".format(df_val.shape[0]))\n",
    "print(\"Samples in test       : {:d}\".format(df_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's score the validation set using only the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_test[\"sentence\"].to_list()\n",
    "results = nlp(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build a DataFrame with the ground truth and the prediction and see how the pretrained model is doing in terms of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(results)\n",
    "results_df[\"label\"] = results_df[\"label\"].replace([\"Neutral\", \"Positive\", \"Negative\"],[0,1,2]) \n",
    "results_df.columns = [\"pred\", \"score\"]\n",
    "results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "results_df = pd.concat([df_test[[\"sentence\", \"label\"]].reset_index(drop=True), results_df], axis=1)\n",
    "\n",
    "results_df[\"Correct\"] = results_df[\"label\"].eq(results_df[\"pred\"])\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the accuracy of the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = len(results_df[results_df[\"Correct\"] == True]) / len(results_df)\n",
    "\n",
    "print(\"Accuracy : {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to keep in mind that this is an imbalanced dataset, so it is good to look at the counts of the classes and the respective accuracy too: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df = pd.concat([results_df[\"label\"].value_counts(), results_df.groupby(\"label\")[\"Correct\"].mean().mul(100).round(2)], axis=1)\n",
    "accuracy_df = accuracy_df.reset_index()\n",
    "accuracy_df.columns = [\"Label\", \"Count\", \"Accuracy\"]\n",
    "accuracy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Fine-tunning\n",
    "\n",
    "The fine-tunning process takes the pretrained model (FinBERT) and performs additional training, tweaking it towards a more specialized use-case. Here, we'll use the training subset of the Sentiment Analysis for Financial News. This transfer learning approach will enables us to produce a more accurate model with a smaller training time.\n",
    "\n",
    "### Datasets preparation\n",
    "\n",
    "First, we need to prepare the three datasets (training, validation, and test) by tokenizing them and by setting the dataset format to be compatible with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:54:33.660010Z",
     "start_time": "2022-06-02T02:54:17.948143Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_val = Dataset.from_pandas(df_val)\n",
    "dataset_test = Dataset.from_pandas(df_test)\n",
    "\n",
    "dataset_train = dataset_train.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", max_length=315), batched=True)\n",
    "dataset_val = dataset_val.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\", max_length=315), batched=True)\n",
    "dataset_test = dataset_test.map(lambda e: tokenizer(e[\"sentence\"], truncation=True, padding=\"max_length\" , max_length=315), batched=True)\n",
    "\n",
    "dataset_train.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])\n",
    "dataset_val.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])\n",
    "dataset_test.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up and training\n",
    "\n",
    "Next, we define the training metrics and some additional customization points like training epochs, size of batches etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:57:15.963784Z",
     "start_time": "2022-06-02T02:54:33.662575Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\" : accuracy_score(predictions, labels)}\n",
    "\n",
    "args = TrainingArguments(\n",
    "        output_dir = \"temp/\",\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        learning_rate=0.00001,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        save_total_limit = 2,\n",
    "        save_strategy = \"no\",\n",
    "        load_best_model_at_end=False,\n",
    "        report_to = \"none\",\n",
    "        optim=\"adamw_torch\")\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=dataset_train,\n",
    "        eval_dataset=dataset_val,\n",
    "        compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform the training.\n",
    "\n",
    "**Note that you will need a hardware tier with sufficient memory and compute, ideally a HW tier which provides GPU acceleration. Otherwise the training process can take a substantial amount of time or crash due to not having access to enough system memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "\n",
    "We can now test the accuracy of the model using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test = trainer.predict(dataset_test).metrics[\"test_accuracy\"]\n",
    "print(\"Accuracy on test: {:.2f}\".format(accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the fine-tuned model\n",
    "\n",
    "Finally, we can save the fine-tuned model and used it for online predictions via a [Model API](https://docs.dominodatalab.com/en/latest/user_guide/8dbc91/host-models-as-rest-apis/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T03:09:11.599174Z",
     "start_time": "2022-06-02T03:09:10.847115Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Please change this location accordingly. You might want to change this depending on whether you are using a git based project\n",
    "or a DFS based project and if you want to use this model\n",
    "''' \n",
    "trainer.save_model(\"/mnt/artifacts/finbert-sentiment/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dca-init": "true",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
